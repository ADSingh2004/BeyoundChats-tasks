# BeyoundChats Content Pipeline - Phase One

A production-ready Node.js web scraper for automatically collecting and storing articles from the BeyondChats blog into MongoDB. Phase 1 focuses on **automated article discovery, web scraping, and persistent storage** with built-in duplicate detection and error handling.

**Status**: âœ… **Fully Operational** - Scrapes 5+ articles per run with MongoDB integration

---

## ğŸ”„ Technical Architecture & Workflow

### System Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USER / TRIGGER                           â”‚
â”‚  (Browser: GET /api/scrape or Scheduled Task)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              EXPRESS SERVER (Port 5000)                     â”‚
â”‚            api/server.js                                    â”‚
â”‚  - Routes HTTP requests                                     â”‚
â”‚  - Connects to MongoDB Atlas                                â”‚
â”‚  - Validates MONGO_URI on startup                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           SCRAPER INITIALIZATION                            â”‚
â”‚      scraper/node-scraper.js                               â”‚
â”‚  1. Detect last page number via pagination                 â”‚
â”‚  2. Collect article URLs from multiple pages               â”‚
â”‚  3. Filter out tag/category pages                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         MULTI-PAGE ARTICLE DISCOVERY                        â”‚
â”‚  - Scan from Page N down to Page N-2 (or until 5 found)    â”‚
â”‚  - Extract article links: /blogs/[article-slug]/           â”‚
â”‚  - EXCLUDE: /tag/, /category/, pagination links            â”‚
â”‚  - Result: Array of article URLs                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      DUPLICATE DETECTION & FILTERING                        â”‚
â”‚  For each article URL:                                      â”‚
â”‚  - Query MongoDB for existing original_url                 â”‚
â”‚  - Skip if already in database (avoid duplicates)          â”‚
â”‚  - Proceed with new articles only                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        ARTICLE CONTENT EXTRACTION                           â”‚
â”‚  For each article URL:                                      â”‚
â”‚  1. Fetch HTML via axios                                   â”‚
â”‚  2. Parse with Cheerio (jQuery-like syntax)                â”‚
â”‚  3. Extract:                                                â”‚
â”‚     - Title from <h1> tag                                  â”‚
â”‚     - Content from .entry-content, .post_content, <article>â”‚
â”‚  4. Validate: Both title and content must exist            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        MONGODB STORAGE & PERSISTENCE                        â”‚
â”‚  Create new Article document:                               â”‚
â”‚  {                                                          â”‚
â”‚    title: "Article Title",                                 â”‚
â”‚    original_url: "https://beyondchats.com/blogs/...",     â”‚
â”‚    original_content: "Full scraped HTML text...",          â”‚
â”‚    status: "Pending",  // Ready for Phase 2 processing    â”‚
â”‚    reference_links: [],  // To be filled in Phase 2       â”‚
â”‚    published_date: "2025-12-31T15:08:48.159Z"             â”‚
â”‚  }                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         API RESPONSE & COMPLETION                           â”‚
â”‚  {"message": "Scraping started/completed. Check logs."}    â”‚
â”‚  Browser/Client receives confirmation                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Execution Flow Example

**Request:**
```bash
curl http://localhost:5000/api/scrape
```

**Server Logs Output:**
```
âœ… MongoDB Connected Successfully
ğŸ”„ Scraper endpoint called...
--- Starting Scraper ---
Detected Last Page: 15
Fetching Page 15: https://beyondchats.com/blogs/page/15/
Found 1 articles on page 15
Fetching Page 14: https://beyondchats.com/blogs/page/14/
Found 9 articles on page 14
Total articles to scrape: 5
Article URLs: [
  'https://beyondchats.com/blogs/introduction-to-chatbots/',
  'https://beyondchats.com/blogs/sales-chatbots/',
  'https://beyondchats.com/blogs/boost-conversion-rate-using-chatbots/',
  'https://beyondchats.com/blogs/common-customer-service-issues/',
  'https://beyondchats.com/blogs/chatbots-vs-live-chat/'
]
Skipping (Already Exists): https://beyondchats.com/blogs/introduction-to-chatbots/
âœ… Saved: From 0 to Sales Hero: How Sales Chatbots Increase Conversions
âœ… Saved: Can Chatbots Boost Your E-commerce Conversions?
âœ… Saved: 10 Solutions for Common Customer Service Issues
âœ… Saved: Chatbots Vs Live Chat: What is best?
--- Scraping Completed ---
```

**Result in Database:**
```json
GET /api/articles returns:
[
  { title: "Chatbots Vs Live Chat: What is best?", url: "..." },
  { title: "10 Solutions for Common Customer Service Issues", url: "..." },
  { title: "Can Chatbots Boost Your E-commerce Conversions?", url: "..." },
  { title: "From 0 to Sales Hero: How Sales Chatbots...", url: "..." },
  { title: "Chatbots Magic: Beginner's Guidebook", url: "..." }
]
```

---

## ğŸ“ Project Structure

```
BeyoundChats-tasks/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ server.js                 # Express server + CRUD endpoints
â”œâ”€â”€ scraper/
â”‚   â””â”€â”€ node-scraper.js           # Multi-page web scraper with filtering
â”œâ”€â”€ models/
â”‚   â””â”€â”€ Article.js                # MongoDB schema (5 fields + timestamps)
â”œâ”€â”€ .env.example                  # Template for MongoDB URI
â”œâ”€â”€ .gitignore                    # Excludes .env, node_modules
â”œâ”€â”€ package.json                  # Dependencies: express, mongoose, axios, cheerio
â””â”€â”€ README.md                    
```

---

## ğŸ› ï¸ Core Components

### 1. **api/server.js** - Express API Server
**Role**: HTTP server and MongoDB gateway

**Key Functions**:
- `mongoose.connect()` - Establishes MongoDB Atlas connection
- `GET /` - Returns API documentation
- `GET /api/scrape` - Triggers web scraper
- `GET /api/articles` - Retrieves all stored articles (sorted by newest)
- `GET /api/articles/:id` - Get single article by MongoDB ID
- `PUT /api/articles/:id` - Update article (for Phase 2)
- `DELETE /api/articles/:id` - Remove article

**Startup Sequence**:
```javascript
1. Load .env (MONGO_URI, PORT)
2. Initialize Express app
3. Connect to MongoDB (logs: "âœ… MongoDB Connected Successfully")
4. Start listening on port 5000
5. Ready for requests
```

### 2. **scraper/node-scraper.js** - Web Scraper Engine
**Role**: Automated article discovery and content extraction

**Execution Steps**:

**Step 1: Pagination Detection**
```javascript
- GET https://beyondchats.com/blogs/
- Parse pagination: Find maximum page number
- Detects: "Last Page: 15"
```

**Step 2: Multi-Page Article Collection**
```javascript
- Iterate from Page 15 â†’ Page 14 â†’ Page 13 (until 5+ articles found)
- For each page:
  * Fetch HTML with axios
  * Parse with Cheerio
  * Extract links matching /blogs/ pattern
  * Exclude: /tag/, /category/, pagination links
- Result: Array of 5 article URLs
```

**Step 3: Duplicate Detection**
```javascript
- For each URL:
  * Query: Article.findOne({ original_url: url })
  * If found: Skip with "Skipping (Already Exists)"
  * If not found: Proceed to scrape
```

**Step 4: Content Extraction**
```javascript
- For each new article:
  * Fetch HTML (axios)
  * Parse HTML (cheerio)
  * Extract title: $article('h1').first().text()
  * Extract content: $article('.entry-content, .post_content, article').text()
  * Validate: Both must exist and be non-empty
```

**Step 5: Database Persistence**
```javascript
- Create Article document:
  * title: "Article Title"
  * original_url: "https://beyondchats.com/blogs/..."
  * original_content: "Full HTML text content"
  * status: "Pending" (for Phase 2 processing)
  * reference_links: [] (empty, filled in Phase 2)
  * published_date: Current timestamp
```

### 3. **models/Article.js** - MongoDB Schema
**Role**: Defines data structure

```javascript
ArticleSchema = {
  title: String,              // Article heading
  original_url: String,       // Source URL (unique index)
  original_content: String,   // Raw scraped HTML text
  updated_content: String,    // Phase 2: Processed content
  status: Enum,              // 'Pending' | 'Processing' | 'Completed' | 'Failed'
  reference_links: [String], // Phase 2: Extracted URLs
  published_date: Date       // Auto-set to current time
}
```

---

## ğŸš€ Getting Started

### Prerequisites
- Node.js v14+
- MongoDB Atlas account (or local MongoDB)
- npm/yarn

### Installation

1. **Clone & Install**:
```bash
git clone https://github.com/ADSingh2004/BeyoundChats-tasks.git
cd BeyoundChats-tasks
npm install
```

2. **Configure MongoDB**:
```bash
cp .env.example .env
```

Edit `.env`:
```env
MONGO_URI=mongodb+srv://username:password@cluster.mongodb.net/content_pipeline?retryWrites=true&w=majority
PORT=5000
```

3. **MongoDB Atlas Setup**:
   - Create cluster on MongoDB Atlas
   - Whitelist IP: Settings â†’ Security â†’ Network Access â†’ Add 0.0.0.0/0
   - Get connection string from "Connect" button
   - Replace username, password in MONGO_URI

### Run Server

**Development** (with auto-restart):
```bash
npm run dev
# OR
nodemon api/server.js
```

**Production**:
```bash
node api/server.js
```

**Output**:
```
Server running on port 5000
âœ… MongoDB Connected Successfully
```

### Test the Scraper

1. **Trigger Scraper** (Browser or cURL):
```bash
curl http://localhost:5000/api/scrape
```

2. **Fetch Articles**:
```bash
curl http://localhost:5000/api/articles | jq .
```

3. **Get Single Article**:
```bash
curl http://localhost:5000/api/articles/{article_id} | jq .
```

---

## ğŸ“Š API Reference

### Endpoints

| Method | Endpoint | Purpose | Response |
|--------|----------|---------|----------|
| GET | `/` | API info & endpoints | `{message, status, endpoints}` |
| GET | `/api/scrape` | Start scraper | `{message: "Scraping..."}` |
| GET | `/api/articles` | Get all articles | `[{Article}]` |
| GET | `/api/articles/:id` | Get article by ID | `{Article}` |
| PUT | `/api/articles/:id` | Update article | `{Article}` |
| DELETE | `/api/articles/:id` | Delete article | `{message}` |

### Example Responses

**GET /api/articles**:
```json
[
  {
    "_id": "507f1f77bcf86cd799439011",
    "title": "From 0 to Sales Hero: How Sales Chatbots...",
    "original_url": "https://beyondchats.com/blogs/sales-chatbots/",
    "original_content": "Full article text...",
    "status": "Pending",
    "reference_links": [],
    "published_date": "2025-12-31T15:08:48.159Z",
    "__v": 0
  }
]
```

---

## ğŸ”§ Configuration

### Environment Variables (.env)

```env
# MongoDB Atlas connection
MONGO_URI=mongodb+srv://user:pass@cluster.mongodb.net/content_pipeline?retryWrites=true&w=majority

# Server port
PORT=5000
```

### Scraper Configuration (node-scraper.js)

**Base URL**:
```javascript
const BASE_URL = 'https://beyondchats.com/blogs/';
```

**Number of Articles**: 
```javascript
const targets = articleLinks.slice(0, 5);  // Change 5 to desired count
```

**Content Selectors**:
```javascript
const content = $article('.entry-content, .post_content, article').text();
// Add/modify selectors if blog HTML changes
```

---

## ğŸ› Error Handling

### MongoDB Connection Error
```
Error: Could not connect to any servers in your MongoDB Atlas cluster
```
**Fix**: 
- Ensure IP whitelist includes your current IP (0.0.0.0/0 for dev)
- Verify MONGO_URI format in .env
- Check network connectivity

### Article Not Saved (No Content)
```
âš ï¸ Skipped (No content): url - Title: "" Content length: 3087
```
**Cause**: Title selector didn't match (likely tag page)
**Solution**: Scraper filters these automatically, no action needed

### HTML Parse Errors
```
Error fetching url: ...
```
**Cause**: Network timeout or invalid URL
**Fix**: Scraper logs and continues; retry if needed

---

## ğŸ”œ Phase 2 (Future)

The `status` field is set to "Pending" for Phase 2 processing:

- **Content Enrichment**: Parse and clean HTML content
- **Reference Extraction**: Find URLs in articles (`reference_links`)
- **Entity Recognition**: Identify key topics/entities
- **Status Update**: Change status to "Processing" â†’ "Completed"
- **Integration**: Connect to AI/NLP services for content enhancement

---

## ğŸ“ˆ Performance & Scalability

### Current Performance
- **Execution Time**: ~15-20 seconds per 5-article run
- **Throughput**: ~60 articles/hour
- **Storage**: ~100KB per article (full HTML)
- **Concurrent Requests**: Single scraper at a time (queuing in Phase 2)

### Optimization Opportunities
- Parallel article fetching (Promise.all)
- Article deduplication by content hash
- Scheduled scraping (cron jobs)
- Caching layer (Redis)
- Database indexing on `original_url`

---

## ğŸ›¡ï¸ Security Considerations

âš ï¸ **Production Deployment**:
- Never commit `.env` file (already in .gitignore)
- Use strong MongoDB credentials
- Rotate API keys regularly
- Implement rate limiting on scraper
- Add request authentication (JWT/API keys)
- Monitor for scraping policy violations

---

## ğŸ“„ License

ISC

---

**Last Updated**: December 31, 2025  
**Phase**: 1 - Web Scraping & Storage (âœ… Complete)  
**Next Phase**: Content Enrichment & Processing
